import openai
import torch
import json


class Model:
    """Class representing an openai model.  Contains low level methods associated with basic model functionality"""

    MAX_CONTEXT_LEN = 500

    def __init__(self, model_name: str):
        """Class representing an openai model.  Contains low level methods associated with basic model functionality

        :param model_name: The name of the OpenAI model to use"""

        self.model_name = model_name
        if not openai.api_key:
            with open("OPENAI_API_KEY.json", "r") as file:
                openai.api_key = json.loads(file.read())["main"]

    def complete_chat(self, prompt: str, context: list[dict[str, str]] = None) -> str:
        """Generates a response to the prompt based off of a proper chat history.  Uses the ChatGPT API only

        :param prompt: The prompt for the model to respond to
        :param context: A list of chats previously generated by the user or model.  This gives context to the model
        :return: ChatGPT's response to the prompt"""

        if self.model_name != "gpt-3.5-turbo":
            raise AttributeError(f"The model type {self.model_name} is not compatible with the chat API.  Try using gpt-3.5-turbo")

        if not context:
            context = []
        if not len(context) or context[0]["role"] != "system":
            context.insert(0, {"role": "system", "content": "You are a helpful assistant."})

        context.append({"role": "user", "content": prompt})

        return openai.ChatCompletion.create(model=self.model_name, messages=context)["choices"][0]["message"]["content"]

    def complete(self, prompt: str, context: list[dict[str, str]] = None, **kwargs) -> str:
        """Completes the prompt with additional context considered.  Uses a fine-tuned model

        :param prompt: The prompt for the model to respond to
        :param context: A list of chats previously generated by the user or model.  This gives context to the model
        :return: The fine-tuned model's response to the prompt"""

        if not self.model_name.startswith("text-"):
            raise AttributeError(f"The model type {self.model_name} is not compatible with the complete API.  Try using a base model")

        if not context:
            return openai.Completion.create(model=self.model_name, prompt=prompt, **kwargs)["choices"][0]["text"]

        # Add context values to prompt so model can include them in current response
        context_prompt = "Please complete using the following information:\n\n"
        for msg in context[::-1]:
            context_prompt = ("Q:" if msg["role"] == "assistant" else "A:")+msg["content"]+"\n"+context_prompt

            # Limit context length to not have too many tokens
            if context_prompt.count(" ") > self.MAX_CONTEXT_LEN:
                break

        context_prompt += f"Q: {prompt}\nA:"

        return openai.Completion.create(model=self.model_name, prompt=context_prompt, **kwargs)["choices"][0]["text"]

    def edit(self, instruction: str, editable: str, **kwargs):
        """Generates a corrected version of the ``editable`` parameter

        :param instruction: An instruction on what the model is to do with the ``editable``
        :param editable: A string of text that the model is to edit
        :return: The model's edit of the text"""

        return openai.Edit.create(model=self.model_name, instruction=instruction, input=editable, **kwargs)["choices"][0]["text"]

    def embed(self, text: str):
        """Creates a vector embedding of the given text

        :param text: The text to embed
        :return: The embedding of the text"""

        return torch.tensor(openai.Embedding.create(model=self.model_name, input=text)["data"]["embedding"], dtype=torch.float)
